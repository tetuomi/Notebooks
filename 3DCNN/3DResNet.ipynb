{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3DResNet.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN5p31l5gQNrS/MaGEvWlBP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QpODaYYjPfdq","executionInfo":{"status":"ok","timestamp":1635238618452,"user_tz":-540,"elapsed":39147,"user":{"displayName":"こんて","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04203319589607435751"}},"outputId":"28f811f7-3d31-434d-cff0-ad605df39913"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qRCpbyA_gSOH","executionInfo":{"status":"ok","timestamp":1635165854781,"user_tz":-540,"elapsed":13451,"user":{"displayName":"こんて","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04203319589607435751"}},"outputId":"5e58adae-7a1e-4213-cb7d-13e30e44307c"},"source":["!pip install pytorchvideo"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pytorchvideo\n","  Downloading pytorchvideo-0.1.3.tar.gz (128 kB)\n","\u001b[K     |████████████████████████████████| 128 kB 5.2 MB/s \n","\u001b[?25hCollecting fvcore\n","  Downloading fvcore-0.1.5.post20211023.tar.gz (49 kB)\n","\u001b[K     |████████████████████████████████| 49 kB 6.0 MB/s \n","\u001b[?25hCollecting av\n","  Downloading av-8.0.3-cp37-cp37m-manylinux2010_x86_64.whl (37.2 MB)\n","\u001b[K     |████████████████████████████████| 37.2 MB 32 kB/s \n","\u001b[?25hCollecting parameterized\n","  Downloading parameterized-0.8.1-py2.py3-none-any.whl (26 kB)\n","Collecting iopath\n","  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorchvideo) (1.19.5)\n","Collecting yacs>=0.1.6\n","  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 47.7 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorchvideo) (4.62.3)\n","Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorchvideo) (1.1.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorchvideo) (7.1.2)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorchvideo) (0.8.9)\n","Collecting portalocker\n","  Downloading portalocker-2.3.2-py2.py3-none-any.whl (15 kB)\n","Building wheels for collected packages: pytorchvideo, fvcore\n","  Building wheel for pytorchvideo (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pytorchvideo: filename=pytorchvideo-0.1.3-py3-none-any.whl size=183829 sha256=e07ec5a104d923a398c0c0810d7fe455146dee41da15e754b0600d861307c419\n","  Stored in directory: /root/.cache/pip/wheels/d4/a7/4c/bada8b1065ae9befac2da6d7f6648cd6718681eb7901ca226d\n","  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fvcore: filename=fvcore-0.1.5.post20211023-py3-none-any.whl size=60947 sha256=4f8b1975cb1b4020da023b5415387eed50c8bb6851b4a57b84bdc0414aa0bb5d\n","  Stored in directory: /root/.cache/pip/wheels/16/98/fc/252d62cab6263c719120e06b28f3378af59b52ce7a20e81852\n","Successfully built pytorchvideo fvcore\n","Installing collected packages: pyyaml, portalocker, yacs, iopath, parameterized, fvcore, av, pytorchvideo\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed av-8.0.3 fvcore-0.1.5.post20211023 iopath-0.1.9 parameterized-0.8.1 portalocker-2.3.2 pytorchvideo-0.1.3 pyyaml-6.0 yacs-0.1.8\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f2MX-A46PqZJ","executionInfo":{"status":"ok","timestamp":1635238632067,"user_tz":-540,"elapsed":714,"user":{"displayName":"こんて","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04203319589607435751"}},"outputId":"6ab6e7c9-5ae8-4947-e596-aa788777e2f5"},"source":["import os\n","print(os.getcwd())\n","os.chdir('/content/drive/My Drive/Google Colab/3DCNN')\n","print(os.getcwd())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n","/content/drive/My Drive/Google Colab/3DCNN\n"]}]},{"cell_type":"code","metadata":{"id":"ixnShPUQP1BJ"},"source":["import random\n","from glob import glob\n","\n","from tqdm.notebook import tqdm\n","import torch\n","import torch.nn as nn\n","import torch.utils.data as data\n","from torchvision.transforms import Compose, Lambda\n","from torchvision.transforms._transforms_video import CenterCropVideo, NormalizeVideo\n","from pytorchvideo.data.encoded_video import EncodedVideo\n","from pytorchvideo.transforms import ApplyTransformToKey, ShortSideScale, UniformTemporalSubsample\n","\n","from models import generate_model\n","from circle_loss import convert_label_to_similarity, CircleLoss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TrPP9_clCNME"},"source":["def load_net(path):\n","    trained_weights = torch.load(path, map_location={'cuda': 'cpu'})\n","    model = generate_model(18)\n","    model.fc = nn.Linear(512, 700)\n","    model.load_state_dict(trained_weights['state_dict'])\n","    net = nn.Sequential(*list(model.children())[:-2]) #avgpool3d入れない\n","    \n","    return net"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"biGkIPcEXmD9"},"source":["# n = load_net('r3d18_K_200ep.pth')\n","# i = torch.zeros(1,3,64,128,128)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HHwmK5O4Dfjl"},"source":["def make_data_path():\n","    data_path = []\n","    for dir_path in glob('./HandWashDataset/*'):\n","        data_path += glob(os.path.join(dir_path, '*.mp4'))\n","    print('DATA NUM: ', len(data_path))\n","    \n","    return data_path"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2I6UzMMBQiCl"},"source":["def data_split(data, train_rate=0.7, val_rate=0.2):\n","    data_num = len(data)\n","    train_num = int(data_num * train_rate)\n","    val_num = int(data_num * val_rate)\n","    return data[:train_num], data[train_num:train_num+val_num], data[train_num+val_num:] "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dbOuEp3mK1aL"},"source":["class LoadDataset(data.Dataset):\n","    def __init__(self, data_path, transform, video_time):\n","        self.data_path = data_path\n","        self.transform = transform\n","        self.video_time = video_time\n","    \n","    def __len__(self):\n","        return len(self.data_path)\n","\n","    def __getitem__(self, index):\n","        video = EncodedVideo.from_path(self.data_path[index])\n","        video_data = video.get_clip(start_sec=0, end_sec=self.video_time)\n","        video_data = self.transform(video_data)\n","        label = int(self.data_path[index][-11:-9])-1\n","\n","        return video_data['video'], label"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ev9YV5jiO_4r"},"source":["def make_dataloader():\n","    data_path = make_data_path()\n","    random.shuffle(data_path)\n","    train_data_path, val_data_path, test_data_path = data_split(data_path)\n","\n","    num_frames = 64\n","    mean = [0.45, 0.45, 0.45]\n","    std = [0.225, 0.225, 0.225]\n","    side_size = 256\n","    crop_size = 256\n","    sampling_rate = 32\n","    frames_per_second = 30\n","    video_time = (num_frames * sampling_rate) / frames_per_second\n","    transform=ApplyTransformToKey(\n","        key='video',\n","        transform=Compose([\n","                UniformTemporalSubsample(num_frames),\n","                Lambda(lambda x: x/255.0),\n","                NormalizeVideo(mean, std),\n","                ShortSideScale(size=side_size),\n","                CenterCropVideo(crop_size=(crop_size, crop_size))\n","        ])\n","    )\n","\n","    train_dataset = LoadDataset(train_data_path, transform, video_time)\n","    val_dataset = LoadDataset(val_data_path, transform, video_time)\n","    test_dataset = LoadDataset(test_data_path, transform, video_time)\n","\n","    batch_size = 1\n","    train_dataloader = data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n","    val_dataloader = data.DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n","    test_dataloader = data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n","\n","    return {'train': train_dataloader, 'val': val_dataloader, 'test': test_dataloader}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j1UUq1-fPvBC","executionInfo":{"status":"ok","timestamp":1635134256029,"user_tz":-540,"elapsed":306,"user":{"displayName":"こんて","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04203319589607435751"}},"outputId":"e7c824fb-d7a1-4a82-8c04-e9c776558b1d"},"source":["d = make_dataloader()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["DATA NUM:  300\n"]}]},{"cell_type":"code","metadata":{"id":"m2kVwISgQp7W"},"source":["t = torch.zeros(3,64,256,256)\n","for v, l in d['train']:\n","    t = v[0]\n","    break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X1e80x1LP0Fq"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wUSdYMNGX2IJ"},"source":["def train(net, dataloader, criterion, optimizer, epochs):\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    print('train on ', device)\n","\n","    net = net.to(device)\n","    if device == 'cuda':\n","        net = torch.nn.DataParallel(net) # make parallel\n","        torch.backends.cudnn.benchmark = True\n","\n","    criterion = criterion.to(device)\n","    \n","    loss_dic = {'train': [], 'val': []}\n","    min_val_loss = -1.0\n","\n","    for epoch in range(epochs):\n","        print('-------------')\n","        print('Epoch {}/{}'.format(epoch+1, epochs))\n","        \n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                net.train()\n","            else:\n","                net.eval()\n","            \n","            epoch_loss = 0.0\n","            \n","            for videos, labels in tqdm(dataloader[phase]):\n","                videos = videos.to(device)\n","                labels = labels.to(device)\n","                \n","                optimizer.zero_grad()\n","\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    outputs = net(videos)\n","\n","                    loss = criterion(*convert_label_to_similarity(outputs, labels))\n","\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","                    \n","                epoch_loss += loss.item() * videos.size(0)\n","\n","            epoch_loss = epoch_loss / len(dataloader[phase].dataset)\n","            loss_dic[phase].append(epoch_loss)\n","            print(f'{phase} loss: {epoch_loss}')\n","\n","            if phase == 'val':\n","                if epoch == 0:\n","                    min_val_loss = epoch_loss\n","                \n","                if epoch_loss < min_val_loss:\n","                    min_val_loss = epoch_loss\n","                    save_path = './3DResNet.pth'\n","                    torch.save(net.state_dict(), save_path)\n","                    print('::::: model is saved :::::')\n","\n","    return loss_dic"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mC051hGDDfii"},"source":["def plot_history_loss(loss):\n","    plt.plot(loss['train'], label=\"loss for training\")\n","    plt.plot(loss['val'], label=\"loss for validation\")\n","    \n","    plt.title('model loss')\n","    plt.xlabel('epoch')\n","    plt.ylabel('loss')\n","    plt.legend(loc='best')\n","    \n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qdbpHN3WDqct"},"source":["def main():\n","    dataloader = make_dataloader()\n","    \n","    model_path = 'r3d18_K_200ep.pth'\n","    net = load_net(model_path)\n","    \n","    criterion = CircleLoss(m=0.25, gamma=80)\n","\n","    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-5)\n","\n","    epochs = 50\n","    loss = train(net, dataloader, criterion, optimizer, epochs)\n","    \n","    plot_history_loss(loss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":337},"id":"Fc1dsNgIJgcv","executionInfo":{"status":"error","timestamp":1635077259886,"user_tz":-540,"elapsed":6458,"user":{"displayName":"こんて","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04203319589607435751"}},"outputId":"c938f47f-5d81-4986-946e-28486c17a598"},"source":["if __name__=='__main__':\n","    torch.manual_seed(1234)\n","    # np.random.seed(1234)\n","    random.seed(1234)\n","    \n","\n","    main()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["DATA NUM:  300\n"]},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-68-b6847d7cbbd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1234\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-67-4c892281374b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mplot_history_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'optimizer' is not defined"]}]},{"cell_type":"code","metadata":{"id":"wcroOtciLGn5"},"source":["import os\n","from typing import Tuple\n","from tqdm import tqdm\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn import manifold\n"," \n","import torch\n","from torch import nn, Tensor\n","from torch import optim\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","import torch.optim.lr_scheduler as lr_scheduler\n","from torchvision.transforms import ToTensor\n","def convert_label_to_similarity(normed_feature: Tensor, label: Tensor) -> Tuple[Tensor, Tensor]:\n","    similarity_matrix = normed_feature @ normed_feature.transpose(1, 0)\n","    label_matrix = label.unsqueeze(1) == label.unsqueeze(0)\n"," \n","    positive_matrix = label_matrix.triu(diagonal=1)\n","    negative_matrix = label_matrix.logical_not().triu(diagonal=1)\n"," \n","    similarity_matrix = similarity_matrix.view(-1)\n","    positive_matrix = positive_matrix.view(-1)\n","    negative_matrix = negative_matrix.view(-1)\n","    return similarity_matrix[positive_matrix], similarity_matrix[negative_matrix]\n"," \n"," \n","class CircleLoss(nn.Module):\n","    def __init__(self, m: float, gamma: float) -> None:\n","        super(CircleLoss, self).__init__()\n","        self.m = m\n","        self.gamma = gamma\n","        self.soft_plus = nn.Softplus()\n"," \n","    def forward(self, sp: Tensor, sn: Tensor) -> Tensor:\n","        ap = torch.clamp_min(- sp.detach() + 1 + self.m, min=0.)\n","        an = torch.clamp_min(sn.detach() + self.m, min=0.)\n"," \n","        delta_p = 1 - self.m\n","        delta_n = self.m\n"," \n","        logit_p = - ap * (sp - delta_p) * self.gamma\n","        logit_n = an * (sn - delta_n) * self.gamma\n"," \n","        loss = self.soft_plus(torch.logsumexp(logit_n, dim=0) + torch.logsumexp(logit_p, dim=0))\n"," \n","        return loss\n","\n","class Model(nn.Module):\n","    def __init__(self) -> None:\n","        super(Model, self).__init__()\n","        self.feature_extractor = nn.Sequential(\n","            nn.Conv2d(in_channels=1, out_channels=8, kernel_size=5),\n","            nn.MaxPool2d(kernel_size=2),\n","            nn.ReLU(),\n","            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=5),\n","            nn.MaxPool2d(kernel_size=2),\n","            nn.ReLU(),\n","            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3),\n","            nn.MaxPool2d(kernel_size=2),\n","            nn.ReLU(),\n","        )\n"," \n","    def forward(self, input: Tensor) -> Tensor:\n","        t = self.feature_extractor(input)\n","        print('feature ', t.shape)\n","        avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n","        feature = t.mean(dim=[2, 3])\n","        print('mean ', feature.shape)\n","        avg = avgpool(t)\n","        print('avg pool ', avg.shape)\n","        print('== ' , avg == feature)\n","        return nn.functional.normalize(feature)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xPZFQxtqWLSL"},"source":["i = torch.zeros(1,1,256,256)\n","m = Model()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wwcY8F5VWjQy","executionInfo":{"status":"ok","timestamp":1635166205267,"user_tz":-540,"elapsed":314,"user":{"displayName":"こんて","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04203319589607435751"}},"outputId":"4eb06aa1-c0ef-40a8-f237-4eed83335a92"},"source":["m = m.to('cpu')\n","m.zero_grad()\n","p = m(i)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["feature  torch.Size([1, 32, 29, 29])\n","mean  torch.Size([1, 32])\n","avg pool  torch.Size([1, 1, 1, 1])\n","==  tensor([[[[False, False, False, False, False, False, False, False, False, False,\n","           False, False, False, False, False, False, False, False, False, False,\n","           False, False, False, False, False, False, False, False, False, False,\n","           False, False]]]])\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hLBfqcY-Wv_C","executionInfo":{"status":"ok","timestamp":1635147939605,"user_tz":-540,"elapsed":321,"user":{"displayName":"こんて","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04203319589607435751"}},"outputId":"d06cd5dd-12c9-48dd-eaf7-459a236d3e42"},"source":["p.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 32])"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","metadata":{"id":"Wt0F6ypoW8rX"},"source":[""],"execution_count":null,"outputs":[]}]}